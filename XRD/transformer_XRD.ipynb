{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selected-event",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from netrc import netrc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Sequential, Linear, ReLU, GRU\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "located-cooper",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available.\")\n",
    "    print(\"PyTorch version:\", torch.__version__)\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "    print(\"Number of available GPUs:\", torch.cuda.device_count())\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mobile-indication",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('....pkl', 'rb') as f:\n",
    "    Data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phantom-subject",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference:https://github.com/chensaian/TransG-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varied-airport",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 num_heads,\n",
    "                 mlp_ratio=4.,\n",
    "                 qkv_bias=False,\n",
    "                 qk_scale=None,\n",
    "                 drop_ratio=0.,\n",
    "                 attn_drop_ratio=0.,\n",
    "                 drop_path_ratio=0.,\n",
    "                 act_layer=nn.GELU,\n",
    "                 norm_layer=nn.LayerNorm):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                              attn_drop_ratio=attn_drop_ratio, proj_drop_ratio=drop_ratio)\n",
    "        self.drop_path = DropPath(\n",
    "            drop_path_ratio) if drop_path_ratio > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim,\n",
    "                       act_layer=act_layer, drop=drop_ratio)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functioning-jewelry",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 dim,  \n",
    "                 num_heads=2,\n",
    "                 qkv_bias=False,\n",
    "                 qk_scale=None,\n",
    "                 attn_drop_ratio=0.,\n",
    "                 proj_drop_ratio=0.):\n",
    "        super(Attention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop_ratio)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop_ratio)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # [batch_size, num_patches + 1, total_embed_dim]\n",
    "        B, N, C = x.shape  \n",
    "        # qkv(): -> [batch_size, num_patches + 1, 3 * total_embed_dim]\n",
    "        # reshape: -> [batch_size, num_patches + 1, 3, num_heads, embed_dim_per_head]\n",
    "        # permute: -> [3, batch_size, num_heads, num_patches + 1, embed_dim_per_head]\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        # [batch_size, num_heads, num_patches + 1, embed_dim_per_head]\n",
    "        # make torchscript happy (cannot use tensor as tuple)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        # transpose: -> [batch_size, num_heads, embed_dim_per_head, num_patches + 1]\n",
    "        # @: multiply -> [batch_size, num_heads, num_patches + 1, num_patches + 1]\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        # @: multiply -> [batch_size, num_heads, num_patches + 1, embed_dim_per_head]\n",
    "        # transpose: -> [batch_size, num_patches + 1, num_heads, embed_dim_per_head]\n",
    "        # reshape: -> [batch_size, num_patches + 1, total_embed_dim]\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dutch-planner",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mlp(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitted-mounting",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def Spectra_Embedding(x, spec_length, embed_dim):\n",
    "    batch_size = x.shape[0]\n",
    "    new_spec_length = (spec_length // embed_dim) * embed_dim\n",
    "    x = x[:, :new_spec_length]\n",
    "    x = torch.reshape(x, (batch_size, spec_length // embed_dim, embed_dim))\n",
    "    return x\n",
    "'''\n",
    "def Spectra_Embedding_old(x, spec_length, embed_dim):\n",
    "\n",
    "    batch_size = x.shape[0]\n",
    "    x = torch.reshape(x, (batch_size, spec_length // embed_dim, embed_dim))  \n",
    "    return x\n",
    "\n",
    "def Spectra_Embedding_enlong(x, spec_length, embed_dim):\n",
    "    batch_size = x.shape[0]\n",
    "    remainder = spec_length % embed_dim\n",
    "    if remainder != 0:\n",
    "        pad = embed_dim - remainder\n",
    "        x = F.pad(x, (0, pad))\n",
    "    x = torch.reshape(x, (batch_size, -1, embed_dim))\n",
    "    return x\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strange-medicine",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VIT(nn.Module):\n",
    "    def __init__(self, spec_length=2000, num_output=1,\n",
    "                 embed_dim=40, depth=12, num_heads=2, mlp_ratio=4.0, qkv_bias=True,\n",
    "                 qk_scale=None, drop_ratio=0.,\n",
    "                 attn_drop_ratio=0., drop_path_ratio=0., norm_layer=None,\n",
    "                 act_layer=None):\n",
    "        \n",
    "        # MSTransformer\n",
    "        super(VIT, self).__init__()\n",
    "        self.num_classes = num_output\n",
    "        self.spec_length = spec_length\n",
    "        self.num_features = self.embed_dim = embed_dim\n",
    "        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n",
    "        act_layer = act_layer or nn.GELU\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, (spec_length//embed_dim) + 1, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=drop_ratio)\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_ratio, depth)]\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            EncoderBlock(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                        drop_ratio=drop_ratio, attn_drop_ratio=attn_drop_ratio, drop_path_ratio=dpr[i],\n",
    "                        norm_layer=norm_layer, act_layer=act_layer)\n",
    "            for i in range(depth)\n",
    "        ])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "        \n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        self.apply(_init_vit_weights)\n",
    "\n",
    "        self.head = nn.Linear(embed_dim, num_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # [B , xrd_length] --> [B , xrd_length/embed_dim , embed_dim]\n",
    "        x = Spectra_Embedding(x, self.spec_length, self.embed_dim)\n",
    "        cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_token, x), dim=1)\n",
    "        x = self.pos_drop(x + self.pos_embed)\n",
    "        x = self.blocks(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.head(x)\n",
    "\n",
    "        return x[:, 0]\n",
    "    \n",
    "def _init_vit_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.trunc_normal_(m.weight, std=.01)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.LayerNorm):\n",
    "        nn.init.zeros_(m.bias)\n",
    "        nn.init.ones_(m.weight)\n",
    "\n",
    "def VIT_model(spec_length=2251,num_output: int = 1):\n",
    "    model = VIT(spec_length=spec_length,\n",
    "                              embed_dim=40,\n",
    "                              depth=12,\n",
    "                              num_heads=2,\n",
    "                              num_output=num_output)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proved-pursuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = Data.iloc[:, 1:].values\n",
    "y = Data.iloc[:, 0].values\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.float)\n",
    "y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "french-cornell",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "num_classes=2000\n",
    "model = VIT_model(spec_length=2251,num_output=num_classes)\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "optimizer = Adam(model.parameters())\n",
    "loss_func = CrossEntropyLoss()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_epochs = 50\n",
    "losses = []\n",
    "early_stop_loss = 0.98 \n",
    "no_improve_epoch = 0 \n",
    "patience = 3 \n",
    "epoch_times = []\n",
    "total_time=0\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()  \n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x_batch)\n",
    "        loss = loss_func(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    end_time = time.time()  \n",
    "    epoch_duration = end_time - start_time  \n",
    "    epoch_times.append(epoch_duration)\n",
    "    total_time+=epoch_duration\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    print(f'Epoch: {epoch+1}, Loss: {epoch_loss}, Time: {epoch_duration:.2f}s,Total time:{total_time}')\n",
    "    \n",
    "    losses.append(epoch_loss)\n",
    "    #print(f'Epoch: {epoch+1}, Loss: {epoch_loss}')\n",
    "\n",
    "\n",
    "    if epoch > 0 and epoch_loss > losses[epoch-1]*early_stop_loss:\n",
    "        no_improve_epoch += 1\n",
    "        print(no_improve_epoch)\n",
    "    else:\n",
    "        no_improve_epoch = 0\n",
    "\n",
    "    if no_improve_epoch > patience:\n",
    "        print(\"Early stopping!\")\n",
    "        break\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "involved-insulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "all_predicted_top1 = []\n",
    "all_predicted_top5 = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    total = 0\n",
    "    correct_top1 = 0\n",
    "    correct_top5 = 0\n",
    "    for x_batch, y_batch in test_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        outputs = model(x_batch)\n",
    "        _, predicted_top1 = torch.max(outputs, 1)\n",
    "        _, predicted_top5 = outputs.topk(5, 1, True, True)\n",
    "        \n",
    "        total += y_batch.size(0)\n",
    "        correct_top1 += (predicted_top1 == y_batch).sum().item()\n",
    "        correct_top5 += predicted_top5.eq(y_batch.view(-1, 1).expand_as(predicted_top5)).sum().item()\n",
    "\n",
    "        all_predicted_top1.extend(predicted_top1.cpu().numpy())\n",
    "        all_predicted_top5.extend(predicted_top5.cpu().numpy())\n",
    "        all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    print(f'Top-1 Accuracy: {correct_top1 / total * 100}%')\n",
    "    print(f'Top-5 Accuracy: {correct_top5 / total * 100}%')\n",
    "\n",
    "\n",
    "for i in range(len(all_labels)):\n",
    "    true_label = label_encoder.inverse_transform([all_labels[i]])[0]\n",
    "    predicted_label_top1 = label_encoder.inverse_transform([all_predicted_top1[i]])[0]\n",
    "    predicted_label_top5 = label_encoder.inverse_transform(all_predicted_top5[i])\n",
    "    print(\"True labels: \", true_label)\n",
    "    print(\"Predicted labels (Top-1): \", predicted_label_top1)\n",
    "    print(\"Predicted labels (Top-5): \", predicted_label_top5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepted-bloom",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_real = pd.read_csv(''test_file_here''.csv')\n",
    "\n",
    "X1 = data_real.iloc[:, 1:].values\n",
    "y1 = data_real.iloc[:, 0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphic-office",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_encoded = label_encoder.transform(y1)\n",
    "\n",
    "X1 = torch.tensor(X1, dtype=torch.float)\n",
    "y1 = torch.tensor(y_encoded, dtype=torch.long)\n",
    "\n",
    "batch_size = 64\n",
    "test_data1 = TensorDataset(X1, y1)\n",
    "test_loader_real = DataLoader(test_data1, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gorgeous-ticket",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predicted_top1 = []\n",
    "all_predicted_top3 = []\n",
    "all_predicted_top5 = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    total = 0\n",
    "    correct_top1 = 0\n",
    "    correct_top3 = 0\n",
    "    correct_top5 = 0\n",
    "    for x_batch, y_batch in test_loader_real:\n",
    "\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        outputs = model(x_batch)\n",
    "        _, predicted_top1 = torch.max(outputs, 1)\n",
    "        _, predicted_top3 = outputs.topk(3, 1, True, True)\n",
    "        _, predicted_top5 = outputs.topk(5, 1, True, True)\n",
    "        \n",
    "        total += y_batch.size(0)\n",
    "        correct_top1 += (predicted_top1 == y_batch).sum().item()\n",
    "        correct_top3 += predicted_top3.eq(y_batch.view(-1, 1).expand_as(predicted_top3)).sum().item()\n",
    "        correct_top5 += predicted_top5.eq(y_batch.view(-1, 1).expand_as(predicted_top5)).sum().item()\n",
    "\n",
    "\n",
    "        all_predicted_top1.extend(predicted_top1.cpu().numpy())\n",
    "        all_predicted_top3.extend(predicted_top3.cpu().numpy())\n",
    "        all_predicted_top5.extend(predicted_top5.cpu().numpy())\n",
    "        all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    print(f'Top-1 Accuracy: {correct_top1 / total * 100}%')\n",
    "    print(f'Top-3 Accuracy: {correct_top3 / total * 100}%')\n",
    "    print(f'Top-5 Accuracy: {correct_top5 / total * 100}%')\n",
    "\n",
    "\n",
    "for i in range(len(all_labels)):\n",
    "    true_label = label_encoder.inverse_transform([all_labels[i]])[0]\n",
    "    predicted_label_top1 = label_encoder.inverse_transform([all_predicted_top1[i]])[0]\n",
    "    predicted_label_top3 = label_encoder.inverse_transform(all_predicted_top3[i].tolist()) \n",
    "    predicted_label_top5 = label_encoder.inverse_transform(all_predicted_top5[i].tolist()) \n",
    "\n",
    "\n",
    "    print(\"------------\")\n",
    "    print(\"True labels: \", true_label)\n",
    "    print(\"Predicted labels (Top-1): \", predicted_label_top1)\n",
    "    print(\"Predicted labels (Top-3): \", predicted_label_top3)\n",
    "    print(\"Predicted labels (Top-5): \", predicted_label_top5)\n",
    "    if true_label not in predicted_label_top1:\n",
    "        print(\"top 1 wrong\")\n",
    "\n",
    "    if true_label not in predicted_label_top3:\n",
    "        print(\"top 3 wrong\")\n",
    "\n",
    "    if true_label not in predicted_label_top5:\n",
    "        print(\"top 5 wrong\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diverse-jurisdiction",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37torch",
   "language": "python",
   "name": "py37torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
