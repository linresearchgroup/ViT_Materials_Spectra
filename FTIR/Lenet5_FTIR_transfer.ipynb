{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.compat.v1.ConfigProto()\n",
    "config = tf.compat.v1.ConfigProto(gpu_options=tf.compat.v1.GPUOptions(allow_growth=True))\n",
    "\n",
    "gpu_options=tf.compat.v1.GPUOptions(allow_growth=True) \n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import pickle\n",
    "import keras\n",
    "import random\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, Flatten\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNGRU, CuDNNLSTM\n",
    "from keras.layers import AveragePooling1D, MaxPooling1D, Bidirectional, GlobalMaxPool1D, Concatenate, GlobalAveragePooling1D, GlobalMaxPooling1D,concatenate\n",
    "from keras.layers import SpatialDropout1D\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "from math import floor\n",
    "import warnings\n",
    "from keras import backend as K\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "random.seed(12345)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"GPU is available.\")\n",
    "    print(\"TensorFlow version:\", tf.__version__)\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    print(\"Number of available GPUs:\", len(gpus))\n",
    "    for i in range(len(gpus)):\n",
    "        print(\"GPU name:\", gpus[i].name)\n",
    "else:\n",
    "    print(\"GPU is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('....pkl', 'rb') as f:\n",
    "    Data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downsize\n",
    "import numpy as np\n",
    "x_original = Data.iloc[:, 1:].values\n",
    "step_size = x_original.shape[1] / 2251\n",
    "new_indices = (np.arange(0, x_original.shape[1], step_size)).astype(int)\n",
    "x_downsampled = x_original[:, new_indices]\n",
    "Data = pd.concat([Data.iloc[:, 0], pd.DataFrame(x_downsampled)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iput_class = 3753\n",
    "oput_class = 3753"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_time=50\n",
    "numtrain = iput_class * aug_time\n",
    "numval = int(0.2 * iput_class)\n",
    "# Extract features and labels\n",
    "X = Data.iloc[:, 1:].values\n",
    "y = Data.iloc[:, 0].values\n",
    "unique_labels = np.unique(y)\n",
    "print(f'There are {len(unique_labels)} unique labels.')\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split your data into training and validation sets:\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert these to TensorFlow tensors\n",
    "X_train = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "y_train = tf.convert_to_tensor(y_train, dtype=tf.int32)\n",
    "X_val = tf.convert_to_tensor(X_val, dtype=tf.float32)\n",
    "y_val = tf.convert_to_tensor(y_val, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "# Construct CNN layers\n",
    "# more para-LeNet-5\n",
    "cnn_model = Sequential()\n",
    "cnn_model.add(Conv1D(6, 5, strides = 1, activation = 'relu', input_shape = (2251, 1)))\n",
    "cnn_model.add(MaxPooling1D(2, strides = 2))\n",
    "cnn_model.add(Dropout(0.2))\n",
    "\n",
    "cnn_model.add(Conv1D(16, 5, strides = 1, activation = 'relu'))\n",
    "cnn_model.add(Conv1D(16, 5, strides = 1, activation = 'relu'))\n",
    "cnn_model.add(MaxPooling1D(2, strides = 2))\n",
    "cnn_model.add(Dropout(0.2))\n",
    "\n",
    "cnn_model.add(Conv1D(32, 5, strides = 1, activation = 'relu'))\n",
    "cnn_model.add(Conv1D(32, 5, strides = 1, activation = 'relu'))\n",
    "cnn_model.add(MaxPooling1D(2, strides = 2))\n",
    "cnn_model.add(Dropout(0.2))\n",
    "\n",
    "cnn_model.add(Conv1D(64, 5, strides = 1, activation = 'relu'))\n",
    "cnn_model.add(Conv1D(64, 5, strides = 1, activation = 'relu'))\n",
    "cnn_model.add(MaxPooling1D(2, strides = 2))\n",
    "cnn_model.add(Dropout(0.2))\n",
    "\n",
    "cnn_model.add(Flatten())\n",
    "cnn_model.add(Dense(120, activation = 'relu'))\n",
    "cnn_model.add(Dense(84, activation = 'relu'))\n",
    "cnn_model.add(Dense(oput_class, activation = 'softmax', activity_regularizer = keras.regularizers.l2(0.1)))\n",
    "cnn_model.compile(loss='categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_to_csv(log_file,early_stopping_setting, epoch, elapsed_time, top1, top3, top5):\n",
    "    # Check if file exists\n",
    "    if not os.path.isfile(log_file):\n",
    "        # Open file in write mode and write headers\n",
    "        with open(log_file, mode='w') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\"early_stopping_setting\",\"Epoch\", \"Elapsed_Time\", \"Top1\", \"Top3\", \"Top5\"])\n",
    "            \n",
    "    # File exists, open in append mode and write data\n",
    "    with open(log_file, mode='a') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([early_stopping_setting,epoch, elapsed_time, top1, top3, top5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert labels to one-hot encoding\n",
    "y_train_onehot = to_categorical(y_train.numpy())\n",
    "y_val_onehot = to_categorical(y_val.numpy())\n",
    "\n",
    "# Set up early stopping\n",
    "#early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
    "class CustomEarlyStopping(Callback):\n",
    "    def __init__(self, patience=0, early_step_setting=0.95):\n",
    "        super(CustomEarlyStopping, self).__init__()\n",
    "        self.patience = patience\n",
    "        self.early_step_setting = early_step_setting\n",
    "        self.wait = 0\n",
    "        self.stopped_epoch = 0\n",
    "        self.best = 0\n",
    "        self.losses = []\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.wait = 0\n",
    "        self.stopped_epoch = 0\n",
    "        self.best = np.Inf\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current_loss = logs.get('loss')\n",
    "        self.losses.append(current_loss)\n",
    "\n",
    "        if epoch > 5 and current_loss > self.losses[epoch - 1] * self.early_step_setting:\n",
    "            self.wait += 1\n",
    "            print('\\n',self.wait)\n",
    "            if self.wait >= self.patience:\n",
    "                self.stopped_epoch = epoch\n",
    "                self.model.stop_training = True\n",
    "                print(\"Early stopping!\")\n",
    "early_stopping_setting=0.9\n",
    "custom_early_stopping = CustomEarlyStopping(patience=3, early_step_setting=early_stopping_setting)\n",
    "start_time = time.time()\n",
    "cnn_model.fit(\n",
    "    X_train.numpy().reshape(X_train.shape[0], X_train.shape[1], 1), \n",
    "    y_train_onehot, \n",
    "    batch_size = 128, \n",
    "    epochs = 20, \n",
    "    verbose = 1,\n",
    "    validation_data=(X_val.numpy().reshape(X_val.shape[0], X_val.shape[1], 1), y_val_onehot),\n",
    "    callbacks=[custom_early_stopping]\n",
    ")\n",
    "elapsed_time = time.time() - start_time\n",
    "'''\n",
    "    # Fit the CNN model\n",
    "cnn_model.fit(\n",
    "    X_train.numpy().reshape(X_train.shape[0], X_train.shape[1], 1), \n",
    "    y_train_onehot, \n",
    "    batch_size = 128, \n",
    "    epochs = 20, \n",
    "    verbose = 1,\n",
    "    validation_data=(X_val.numpy().reshape(X_val.shape[0], X_val.shape[1], 1), y_val_onehot),\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "'''\n",
    "    \n",
    "def calculate_top_k_accuracy(y_true, y_pred, k):\n",
    "    top_k_preds = np.argsort(y_pred, axis=-1)[:, -k:]\n",
    "    target_in_top_k = [(true in top_k) for true, top_k in zip(y_true, top_k_preds)]\n",
    "    return np.mean(target_in_top_k)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = cnn_model.predict(X_val.numpy().reshape(X_val.shape[0], X_val.shape[1], 1))\n",
    "\n",
    "# Calculate top-1, top-3, top-5 accuracies\n",
    "top_1_accuracy = calculate_top_k_accuracy(y_val.numpy(), y_pred, 1)\n",
    "top_3_accuracy = calculate_top_k_accuracy(y_val.numpy(), y_pred, 3)\n",
    "top_5_accuracy = calculate_top_k_accuracy(y_val.numpy(), y_pred, 5)\n",
    "\n",
    "print(f\"Top-1 Accuracy: {round(top_1_accuracy, 2) * 100}%\")\n",
    "print(f\"Top-3 Accuracy: {round(top_3_accuracy, 2) * 100}%\")\n",
    "print(f\"Top-5 Accuracy: {round(top_5_accuracy, 2) * 100}%\")\n",
    "\n",
    "# Plot training loss\n",
    "plt.plot(custom_early_stopping.losses)\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "top_1_accuracies = []\n",
    "top_3_accuracies = []\n",
    "top_5_accuracies = []\n",
    "total_times = []\n",
    "num_classes=3753\n",
    "for iiiii in range(10):\n",
    "    keras.backend.clear_session()\n",
    "    # Construct CNN layers\n",
    "    # more para-LeNet-5\n",
    "    pretrained_model = keras.models.load_model('CNNmodel_test_top1_accuracy=0.97_test_top3_accuracy=1.0_test_top5_accuracy=1.0_test_current_time=20230619-224155.h5')\n",
    "\n",
    "    # Remove the last layer\n",
    "    pretrained_model._layers.pop()\n",
    "\n",
    "    cnn_model = keras.models.Sequential()\n",
    "    for layer in pretrained_model.layers[:-1]:  # just exclude last layer from copying\n",
    "        cnn_model.add(layer)\n",
    "\n",
    "    # Add a new output layer to the new model\n",
    "    new_output_layer = keras.layers.Dense(num_classes, activation='softmax', name='unique_dense_layer')\n",
    "    cnn_model.add(new_output_layer)\n",
    "    cnn_model.compile(loss='categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    \n",
    "\n",
    "    y_train_onehot = to_categorical(y_train)\n",
    "    y_val_onehot = to_categorical(y_val)\n",
    "\n",
    "    # Set up early stopping\n",
    "    #early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
    "\n",
    "\n",
    "    early_stopping_setting=0.98\n",
    "    custom_early_stopping = CustomEarlyStopping(patience=3, early_step_setting=early_stopping_setting)\n",
    "    start_time = time.time()\n",
    "    cnn_model.fit(\n",
    "    X_train.numpy().reshape(X_train.shape[0], X_train.shape[1], 1), \n",
    "    y_train_onehot, \n",
    "    batch_size = 128, \n",
    "    epochs = 50, \n",
    "    verbose = 1,\n",
    "    validation_data=(X_val.numpy().reshape(X_val.shape[0], X_val.shape[1], 1), y_val_onehot),\n",
    "    callbacks=[custom_early_stopping]\n",
    "    )\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    with open('FTIR_test_data.pkl', 'rb') as f:\n",
    "        Test = pickle.load(f)\n",
    "    X_test = Test.iloc[:, 1:].values\n",
    "    X_test.shape\n",
    "    X_test_resampled = X_test[:, new_indices]\n",
    "    X_test_resampled.shape\n",
    "    df_resampled = pd.DataFrame(X_test_resampled, index=Test.index)\n",
    "    X_test = df_resampled.values\n",
    "    y_true = Test.iloc[:,0]\n",
    "    unique_labels_test = np.unique(y_true)\n",
    "    missing_labels = set(unique_labels_test).difference(set(unique_labels))\n",
    "    y_true_series = pd.Series(y_true)\n",
    "    mask = ~y_true_series.isin(missing_labels)\n",
    "    X_test = X_test[mask.values]\n",
    "    y_true = y_true_series[mask].values\n",
    "    # Make predictions\n",
    "    y_pred = cnn_model.predict(X_test.reshape(len(y_true), 2251, 1))\n",
    "\n",
    "    # Get the top k predictions\n",
    "    top_1_preds = np.argsort(y_pred, axis=-1)[:, -1:]\n",
    "    top_3_preds = np.argsort(y_pred, axis=-1)[:, -3:]\n",
    "    top_5_preds = np.argsort(y_pred, axis=-1)[:, -5:]\n",
    "\n",
    "    # Get the true labels in encoded format\n",
    "    true_labels_encoded = label_encoder.transform(y_true)\n",
    "\n",
    "\n",
    "    # Initialize counters\n",
    "    correct_top1 = 0\n",
    "    correct_top3 = 0\n",
    "    correct_top5 = 0\n",
    "    total = len(true_labels_encoded)\n",
    "\n",
    "    # First loop to calculate accuracy\n",
    "    for i, true_label_encoded in enumerate(true_labels_encoded):\n",
    "        true_label = label_encoder.inverse_transform([true_label_encoded])[0]\n",
    "\n",
    "        # Decode the predicted labels\n",
    "        predicted_label_top1 = label_encoder.inverse_transform(top_1_preds[i])[0]\n",
    "        predicted_label_top3 = label_encoder.inverse_transform(top_3_preds[i])\n",
    "        predicted_label_top5 = label_encoder.inverse_transform(top_5_preds[i])\n",
    "\n",
    "        if true_label == predicted_label_top1:\n",
    "            correct_top1 += 1\n",
    "\n",
    "        if true_label in predicted_label_top3:\n",
    "            correct_top3 += 1\n",
    "\n",
    "        if true_label in predicted_label_top5:\n",
    "            correct_top5 += 1\n",
    "\n",
    "    # Print accuracy\n",
    "    print(f'Top-1 Accuracy: {correct_top1 / total * 100}%')\n",
    "    print(f'Top-3 Accuracy: {correct_top3 / total * 100}%')\n",
    "    print(f'Top-5 Accuracy: {correct_top5 / total * 100}%')\n",
    "    # Append the results\n",
    "    top_1_accuracies.append(correct_top1 / total * 100)\n",
    "    top_3_accuracies.append(correct_top3 / total * 100)\n",
    "    top_5_accuracies.append(correct_top5 / total * 100)\n",
    "    total_times.append(elapsed_time)\n",
    "# Calculate mean and std for accuracies and elapsed time\n",
    "mean_top_1_accuracy = np.mean(top_1_accuracies)\n",
    "std_top_1_accuracy = np.std(top_1_accuracies)\n",
    "\n",
    "mean_top_3_accuracy = np.mean(top_3_accuracies)\n",
    "std_top_3_accuracy = np.std(top_3_accuracies)\n",
    "\n",
    "mean_top_5_accuracy = np.mean(top_5_accuracies)\n",
    "std_top_5_accuracy = np.std(top_5_accuracies)\n",
    "\n",
    "mean_time = np.mean(total_times)\n",
    "std_time = np.std(total_times)\n",
    "\n",
    "# Create a dataframe\n",
    "results_df = pd.DataFrame({\n",
    "    'Top_1_Accuracy': top_1_accuracies,\n",
    "    'Top_3_Accuracy': top_3_accuracies,\n",
    "    'Top_5_Accuracy': top_5_accuracies,\n",
    "    'Elapsed_Time': total_times\n",
    "})\n",
    "\n",
    "# Add mean and std rows\n",
    "results_df = results_df.append({\n",
    "    'Top_1_Accuracy': mean_top_1_accuracy,\n",
    "    'Top_3_Accuracy': mean_top_3_accuracy,\n",
    "    'Top_5_Accuracy': mean_top_5_accuracy,\n",
    "    'Elapsed_Time': mean_time\n",
    "}, ignore_index=True)\n",
    "\n",
    "results_df = results_df.append({\n",
    "    'Top_1_Accuracy': std_top_1_accuracy,\n",
    "    'Top_3_Accuracy': std_top_3_accuracy,\n",
    "    'Top_5_Accuracy': std_top_5_accuracy,\n",
    "    'Elapsed_Time': std_time\n",
    "}, ignore_index=True)\n",
    "\n",
    "# Save the dataframe to a csv file\n",
    "results_df.to_csv('....csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
