{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "traditional-stations",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "killing-return",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize empty list to store dictionaries of data from each file\n",
    "data = []\n",
    "\n",
    "# function to process individual file\n",
    "def process_file(path, filename):\n",
    "    try:\n",
    "        with open(os.path.join(path, filename)) as f:\n",
    "            lines = f.readlines()\n",
    "    except UnicodeDecodeError:\n",
    "        print('Skipping non-UTF8 file: ', filename)\n",
    "        return   \n",
    "    \n",
    "    # Check if the file starts with '##'\n",
    "    if not lines[0].startswith('##'):\n",
    "        return\n",
    "    \n",
    "    \n",
    "    \n",
    "    file_data = {}\n",
    "    file_data['filename'] = filename\n",
    "    #file_data['title'] = lines[0].split(\"=\")[1].split(\"\\n\")[0]\n",
    "    n = 0\n",
    "    found_xydata = False\n",
    "    for line in lines:\n",
    "        n += 1\n",
    "        if \"TITLE\" in line:\n",
    "            file_data['label'] = line.split(\"=\")[1].split(\"\\n\")[0]\n",
    "        if \"XUNITS\" in line:\n",
    "            file_data['xunits'] = line.split(\"=\")[1].split(\"\\n\")[0]\n",
    "        if \"YUNITS\" in line:\n",
    "            file_data['yunits'] = line.split(\"=\")[1].split(\"\\n\")[0]    \n",
    "        if \"XYDATA\" in line:\n",
    "            found_xydata = True\n",
    "            break\n",
    "    # Skip the file if it does not contain XYDATA\n",
    "    if not found_xydata:\n",
    "        return\n",
    "    \n",
    "    print(filename)\n",
    "    df = pd.read_csv(os.path.join(path, filename), header=None, sep=' ', skiprows=n+1, skipfooter=2, engine='python')\n",
    "\n",
    "    file_data['xdata'] = df[0].tolist()\n",
    "    file_data['ydata'] = [df[i].tolist() for i in range(1, len(df.columns))]\n",
    "\n",
    "    data.append(file_data)\n",
    "\n",
    "# function to process all files in specified directory and its subdirectories\n",
    "def process_directory(path):\n",
    "    for filename in os.listdir(path):\n",
    "        if os.path.isfile(os.path.join(path, filename)):\n",
    "            process_file(path, filename)\n",
    "        elif os.path.isdir(os.path.join(path, filename)):\n",
    "            process_directory(os.path.join(path, filename))\n",
    "\n",
    "# start processing from current directory\n",
    "process_directory('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rotary-fifth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates from data based on the title (label)\n",
    "unique_data = []\n",
    "unique_titles = set()\n",
    "for d in data:\n",
    "    if d['label'] not in unique_titles:\n",
    "        unique_titles.add(d['label'])\n",
    "        unique_data.append(d)\n",
    "\n",
    "# Count the number of XY pairs for each label\n",
    "xy_pair_counts = {}\n",
    "for d in unique_data:\n",
    "    xy_pairs = len(d['ydata'])\n",
    "    xy_pair_counts[d['label']] = xy_pairs\n",
    "\n",
    "# Print the number of XY pairs for each label\n",
    "for label, count in xy_pair_counts.items():\n",
    "    print(f\"Label: {label}, Number of XY pairs: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thrown-equipment",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import interpolate\n",
    "import pandas as pd\n",
    "\n",
    "# Interpolation on X direction\n",
    "x_new = np.arange(700, 3500, 1)\n",
    "\n",
    "interpolated_data = []\n",
    "for d in unique_data:\n",
    "    temp_xx = np.array(d['xdata'])\n",
    "    \n",
    "    # Convert units\n",
    "    if d['xunits'] == 'MICROMETERS':\n",
    "        temp_xx = 10000 / temp_xx\n",
    "    \n",
    "    # If the X values are in descending order, flip X values\n",
    "    if temp_xx[0] > temp_xx[-1]:\n",
    "        temp_xx = np.flip(temp_xx)\n",
    "    \n",
    "    for y_data in d['ydata']:\n",
    "        temp_yy = np.array(y_data)\n",
    "        \n",
    "        # Convert units\n",
    "        temp_yy = temp_yy / max(temp_yy)\n",
    "        if d['yunits'] == 'TRANSMITTANCE':\n",
    "            temp_yy = 1 - temp_yy\n",
    "            \n",
    "        # If the X values are in descending order, flip Y values\n",
    "        if temp_xx[0] > temp_xx[-1]:\n",
    "            temp_yy = np.flip(temp_yy)\n",
    "    \n",
    "        # Interpolate the Y values\n",
    "        tck = interpolate.splrep(temp_xx, temp_yy)\n",
    "        y_bspline = interpolate.splev(x_new, tck)\n",
    "    \n",
    "        # Append to the list\n",
    "        interpolated_data.append({'label': d['label'], 'ydata': y_bspline})\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame()\n",
    "for d in interpolated_data:\n",
    "    temp_df = pd.DataFrame(d['ydata']).T\n",
    "    temp_df.insert(0, 'label', d['label'])\n",
    "    df = df.append(temp_df)\n",
    "\n",
    "# Reset index\n",
    "df.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chinese-buying",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-satellite",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defined-stereo",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-plane",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize train and test data lists\n",
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "# Split test data and train data\n",
    "for label in df['label'].unique():\n",
    "    label_df = df[df['label'] == label]\n",
    "    test_data_sample = label_df.sample(n=1)\n",
    "    train_data_sample = label_df.drop(test_data_sample.index)\n",
    "    \n",
    "    # Append to train and test data lists\n",
    "    train_data.append(train_data_sample)\n",
    "    test_data.append(test_data_sample)\n",
    "\n",
    "# Convert lists of dataframes into single dataframes\n",
    "train_df = pd.concat(train_data)\n",
    "test_df = pd.concat(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amateur-turkey",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_aug_function1(row):\n",
    "    label = row['label']\n",
    "    theoretical = row.values[1:]\n",
    "    #print(theoretical)\n",
    "    copy_theo = np.copy(theoretical)  # Create a copy of the original data to avoid modifying it directly\n",
    "\n",
    "    # Replace the peak-specific operations with the random elimination and scaling processes\n",
    "    # Random elimination\n",
    "    dum1 = np.repeat(np.random.choice([0, 1], 300, p=[0.2, 0.8]), len(copy_theo) // 300)\n",
    "    dum1 = np.append(dum1, np.zeros([len(copy_theo) - len(dum1), ]))\n",
    "    copy_theo = np.multiply(copy_theo, dum1)\n",
    "\n",
    "    # Random scaling\n",
    "    dum2 = np.repeat(np.random.rand(150,), len(copy_theo) // 150)\n",
    "    dum2 = np.append(dum2, np.zeros([len(copy_theo) - len(dum2), ]))\n",
    "    copy_theo = np.multiply(copy_theo, dum2)\n",
    "\n",
    "    # Normalize the data\n",
    "    copy_theo_elimination_scaling = copy_theo\n",
    "    copy_theo_elimination_scaling = (copy_theo_elimination_scaling - np.min(copy_theo_elimination_scaling)) / (\n",
    "        np.max(copy_theo_elimination_scaling) - np.min(copy_theo_elimination_scaling) + 1e-9)\n",
    "\n",
    "    # Add noise\n",
    "    #noise_intensity = np.random.uniform(0, 0.005)\n",
    "    noise_intensity=0\n",
    "    noise = np.random.normal(0, noise_intensity * np.max(copy_theo_elimination_scaling), len(copy_theo_elimination_scaling))\n",
    "    copy_theo_elimination_scaling += noise\n",
    "\n",
    "    # Normalize again\n",
    "    copy_theo_elimination_scaling = (copy_theo_elimination_scaling - np.min(copy_theo_elimination_scaling)) / (\n",
    "        np.max(copy_theo_elimination_scaling) - np.min(copy_theo_elimination_scaling) + 1e-9)\n",
    "\n",
    "    # Left-right shifting process\n",
    "    shift = np.random.randint(-25 * 1, 25)  # Generate a random integer between -20 and 20 for left-right shifting\n",
    "    if shift >= 0:\n",
    "        # Shift data to the right\n",
    "        copy_theo_elimination_scaling = np.append(copy_theo_elimination_scaling[shift:], np.zeros([shift,]))\n",
    "    else:\n",
    "        # Shift data to the left\n",
    "        copy_theo_elimination_scaling = np.append(copy_theo_elimination_scaling[0:len(copy_theo) + shift],\n",
    "                                                  np.zeros([shift * -1,]))\n",
    "\n",
    "    # Construct augmented row data with label\n",
    "    augmented_row = np.insert(copy_theo_elimination_scaling, 0, label)\n",
    "    return pd.Series(augmented_row, index=row.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "requested-cholesterol",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our Augmentation Method\n",
    "def custom_aug_function2(row):\n",
    "    label = row[0]\n",
    "    theoretical = row[1:]\n",
    "\n",
    "    copy_theo = np.copy(theoretical)\n",
    "\n",
    "    # Random elimination\n",
    "    dum1 = []\n",
    "    while len(dum1) < len(copy_theo):\n",
    "        repeat_count = np.random.randint(7, 9)\n",
    "        dum1 = np.append(dum1, np.repeat(np.random.choice([0, 1], 1, p=[0.2, 0.8]), repeat_count))\n",
    "    dum1 = dum1[:len(copy_theo)]  # if dum1 is longer than copy_theo, truncate it\n",
    "    copy_theo = np.multiply(copy_theo, dum1)\n",
    "\n",
    "    # Random scaling\n",
    "    dum2 = []\n",
    "    while len(dum2) < len(copy_theo):\n",
    "        repeat_count = np.random.randint(14, 17)\n",
    "        dum2 = np.append(dum2, np.repeat(np.random.rand(1), repeat_count))\n",
    "    dum2 = dum2[:len(copy_theo)]  # if dum2 is longer than copy_theo, truncate it\n",
    "    copy_theo = np.multiply(copy_theo, dum2)\n",
    "\n",
    "    # Normalize the data\n",
    "    copy_theo_elimination_scaling = copy_theo\n",
    "    copy_theo_elimination_scaling = (copy_theo_elimination_scaling - np.min(copy_theo_elimination_scaling)) / (\n",
    "        np.max(copy_theo_elimination_scaling) - np.min(copy_theo_elimination_scaling) + 1e-9)\n",
    "\n",
    "    # Add noise\n",
    "    noise_intensity = 0\n",
    "    noise = np.random.normal(0, noise_intensity * np.max(copy_theo_elimination_scaling), len(copy_theo_elimination_scaling))\n",
    "    copy_theo_elimination_scaling += noise\n",
    "\n",
    "    # Normalize again\n",
    "    copy_theo_elimination_scaling = (copy_theo_elimination_scaling - np.min(copy_theo_elimination_scaling)) / (\n",
    "        np.max(copy_theo_elimination_scaling) - np.min(copy_theo_elimination_scaling) + 1e-9)\n",
    "\n",
    "    # Left-right shifting process\n",
    "    shift = np.random.randint(-25, 25)  # Generate a random integer between -25 and 25 for left-right shifting\n",
    "    if shift >= 0:\n",
    "        # Shift data to the right\n",
    "        copy_theo_elimination_scaling = np.append(copy_theo_elimination_scaling[shift:], np.zeros([shift,]))\n",
    "    else:\n",
    "        # Shift data to the left\n",
    "        copy_theo_elimination_scaling = np.append(np.zeros([abs(shift),]), copy_theo_elimination_scaling[:len(copy_theo) + shift])\n",
    "\n",
    "    # Construct augmented row data with label\n",
    "    augmented_row = np.insert(copy_theo_elimination_scaling, 0, label)\n",
    "    return augmented_row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honest-parade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target number of data for each label\n",
    "target_num = 50\n",
    "# Get unique labels\n",
    "\n",
    "unique_labels = train_df['label'].unique()\n",
    "\n",
    "# Create a dictionary to store the positions of each label in train_df\n",
    "label_positions = {label: np.where(train_df['label'] == label)[0] for label in unique_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proof-haven",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Augment the train data\n",
    "augmented_data_list = []  # Use a list to store augmented data\n",
    "# Loop over each unique label\n",
    "for label in tqdm(unique_labels, desc=\"Processing labels\"):    \n",
    "    # Get the positions of current label\n",
    "    label_pos = label_positions[label]\n",
    "    \n",
    "    # Use custom_aug_function to augment data\n",
    "    for i in range(target_num):\n",
    "        # Randomly select a position from label_pos and get the corresponding row in train_df\n",
    "        sample_to_augment = train_df.iloc[np.random.choice(label_pos)]\n",
    "        #print(sample_to_augment)\n",
    "        augmented_data = custom_aug_function2(sample_to_augment)\n",
    "        augmented_data_list.append(augmented_data)  # Append the data to the list\n",
    "\n",
    "# Convert the list of augmented data to a DataFrame\n",
    "augmented_df = pd.DataFrame(augmented_data_list, columns=train_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understood-amplifier",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new columns order with 'label' at the start\n",
    "new_columns_order = ['label'] + [col for col in augmented_df.columns if col != 'label']\n",
    "\n",
    "# Reindex the DataFrame\n",
    "augmented_df = augmented_df.loc[:, new_columns_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boring-adapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the train and test dataframes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37torch",
   "language": "python",
   "name": "py37torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
