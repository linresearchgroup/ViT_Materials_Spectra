{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selected-event",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from netrc import netrc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Sequential, Linear, ReLU, GRU\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "located-cooper",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available.\")\n",
    "    print(\"PyTorch version:\", torch.__version__)\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "    print(\"Number of available GPUs:\", torch.cuda.device_count())\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mobile-indication",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('FTIR_train_data_class3761_aug50.pkl', 'rb') as f:\n",
    "    Data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "golden-equivalent",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_arr = np.unique(y)\n",
    "\n",
    "# Check unique labels in dataset\n",
    "print(\"Unique values:\", unique_arr)\n",
    "print(\"Number of unique values:\", len(unique_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exceptional-abraham",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference:https://github.com/chensaian/TransG-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varied-airport",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 num_heads,\n",
    "                 mlp_ratio=4.,\n",
    "                 qkv_bias=False,\n",
    "                 qk_scale=None,\n",
    "                 drop_ratio=0.,\n",
    "                 attn_drop_ratio=0.,\n",
    "                 drop_path_ratio=0.,\n",
    "                 act_layer=nn.GELU,\n",
    "                 norm_layer=nn.LayerNorm):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                              attn_drop_ratio=attn_drop_ratio, proj_drop_ratio=drop_ratio)\n",
    "        self.drop_path = DropPath(\n",
    "            drop_path_ratio) if drop_path_ratio > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim,\n",
    "                       act_layer=act_layer, drop=drop_ratio)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functioning-jewelry",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 dim,  \n",
    "                 num_heads=2,\n",
    "                 qkv_bias=False,\n",
    "                 qk_scale=None,\n",
    "                 attn_drop_ratio=0.,\n",
    "                 proj_drop_ratio=0.):\n",
    "        super(Attention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop_ratio)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop_ratio)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # [batch_size, num_patches + 1, total_embed_dim]\n",
    "        B, N, C = x.shape  \n",
    "        # qkv(): -> [batch_size, num_patches + 1, 3 * total_embed_dim]\n",
    "        # reshape: -> [batch_size, num_patches + 1, 3, num_heads, embed_dim_per_head]\n",
    "        # permute: -> [3, batch_size, num_heads, num_patches + 1, embed_dim_per_head]\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        # [batch_size, num_heads, num_patches + 1, embed_dim_per_head]\n",
    "        # make torchscript happy (cannot use tensor as tuple)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        # transpose: -> [batch_size, num_heads, embed_dim_per_head, num_patches + 1]\n",
    "        # @: multiply -> [batch_size, num_heads, num_patches + 1, num_patches + 1]\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        self.attn_weights = attn.detach() \n",
    "        attn = self.attn_drop(attn)\n",
    "        # @: multiply -> [batch_size, num_heads, num_patches + 1, embed_dim_per_head]\n",
    "        # transpose: -> [batch_size, num_patches + 1, num_heads, embed_dim_per_head]\n",
    "        # reshape: -> [batch_size, num_patches + 1, total_embed_dim]\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "\n",
    "        \n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dutch-planner",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mlp(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitted-mounting",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def Spectra_Embedding(x, spec_length, embed_dim):\n",
    "    batch_size = x.shape[0]\n",
    "    new_spec_length = (spec_length // embed_dim) * embed_dim\n",
    "    x = x[:, :new_spec_length]\n",
    "    x = torch.reshape(x, (batch_size, spec_length // embed_dim, embed_dim))\n",
    "    return x\n",
    "'''\n",
    "def Spectra_Embedding_old(x, spec_length, embed_dim):\n",
    "\n",
    "    batch_size = x.shape[0]\n",
    "    x = torch.reshape(x, (batch_size, spec_length // embed_dim, embed_dim))  \n",
    "    return x\n",
    "\n",
    "def Spectra_Embedding_enlong(x, spec_length, embed_dim):\n",
    "    batch_size = x.shape[0]\n",
    "    remainder = spec_length % embed_dim\n",
    "    if remainder != 0:\n",
    "        pad = embed_dim - remainder\n",
    "        x = F.pad(x, (0, pad))\n",
    "    x = torch.reshape(x, (batch_size, -1, embed_dim))\n",
    "    return x\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strange-medicine",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VIT(nn.Module):\n",
    "    def __init__(self, spec_length=2000, num_output=1,\n",
    "                 embed_dim=40, depth=12, num_heads=2, mlp_ratio=4.0, qkv_bias=True,\n",
    "                 qk_scale=None, drop_ratio=0.,\n",
    "                 attn_drop_ratio=0., drop_path_ratio=0., norm_layer=None,\n",
    "                 act_layer=None):\n",
    "        \n",
    "        super(VIT, self).__init__()\n",
    "        self.num_classes = num_output\n",
    "        self.spec_length = spec_length\n",
    "        self.num_features = self.embed_dim = embed_dim\n",
    "        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n",
    "        act_layer = act_layer or nn.GELU\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, (spec_length//embed_dim) + 1, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=drop_ratio)\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_ratio, depth)]\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            EncoderBlock(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                        drop_ratio=drop_ratio, attn_drop_ratio=attn_drop_ratio, drop_path_ratio=dpr[i],\n",
    "                        norm_layer=norm_layer, act_layer=act_layer)\n",
    "            for i in range(depth)\n",
    "        ])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "        \n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        self.apply(_init_vit_weights)\n",
    "\n",
    "        self.head = nn.Linear(embed_dim, num_output)\n",
    "    def get_attention_maps(self):\n",
    "        attention_maps = []\n",
    "        for block in self.blocks:\n",
    "            attention_maps.append(block.attn.attn_weights)\n",
    "        return attention_maps\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # [B , xrd_length] --> [B , xrd_length/embed_dim , embed_dim]\n",
    "        x = Spectra_Embedding(x, self.spec_length, self.embed_dim)\n",
    "        cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_token, x), dim=1)\n",
    "        x = self.pos_drop(x + self.pos_embed)\n",
    "        x = self.blocks(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.head(x)\n",
    "\n",
    "        return x[:, 0]\n",
    "    \n",
    "def _init_vit_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.trunc_normal_(m.weight, std=.01)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.LayerNorm):\n",
    "        nn.init.zeros_(m.bias)\n",
    "        nn.init.ones_(m.weight)\n",
    "\n",
    "def VIT_model(spec_length=2251,num_output: int = 1,embed_dim=80, depth=6,num_heads=4):\n",
    "    model = VIT(spec_length=spec_length,\n",
    "                              embed_dim=embed_dim,\n",
    "                              depth=depth,\n",
    "                              num_heads=num_heads,\n",
    "                              num_output=num_output)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proved-pursuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Data.iloc[:, 1:].values\n",
    "y = Data.iloc[:, 0].values\n",
    "unique_labels = np.unique(y)\n",
    "\n",
    "print(f'There are {len(unique_labels)} unique labels.')\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.float)\n",
    "y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "batch_size = 1024\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interior-investigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "french-cornell",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "num_classes=3753\n",
    "embed_dim=480\n",
    "depth=10\n",
    "num_heads=10\n",
    "\n",
    "pretrained_model_path = \"model_embed_dim=480_depth=10_early_step_setting=0.88_num_head=1_test_top1_accuracy=73.33333333333333_test_top3_accuracy=100.0_test_top5_accuracy=100.0_test_current_time=2023-05-28 01:57:38.860881.pt\"\n",
    "pretrained_dict = torch.load(pretrained_model_path)\n",
    "\n",
    "model = VIT_model(spec_length=2251, num_output=num_classes, embed_dim=embed_dim, depth=depth,num_heads=num_heads)\n",
    "model_dict = model.state_dict()\n",
    "\n",
    "pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and 'head' not in k}\n",
    "\n",
    "model_dict.update(pretrained_dict)\n",
    "model.load_state_dict(model_dict)\n",
    "\n",
    "model.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = Adam(model.parameters())\n",
    "loss_func = CrossEntropyLoss()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_epochs = 100\n",
    "losses = []\n",
    "early_stop_loss = 0.90 \n",
    "no_improve_epoch = 0 \n",
    "patience = 3 \n",
    "epoch_times = []\n",
    "total_time=0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()  \n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x_batch)\n",
    "        loss = loss_func(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    end_time = time.time()  \n",
    "    epoch_duration = end_time - start_time \n",
    "    epoch_times.append(epoch_duration)\n",
    "    total_time+=epoch_duration\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    print(f'Epoch: {epoch+1}, Loss: {epoch_loss:.10f}, Time: {epoch_duration:.2f}s, Total time:{total_time:.2f}s')\n",
    "    \n",
    "    losses.append(epoch_loss)\n",
    "    #print(f'Epoch: {epoch+1}, Loss: {epoch_loss}')\n",
    "\n",
    "   \n",
    "    if epoch > 4 and epoch_loss > losses[epoch-1]*early_stop_loss:\n",
    "        no_improve_epoch += 1\n",
    "        print(no_improve_epoch)\n",
    "    else:\n",
    "        no_improve_epoch = 0\n",
    "\n",
    "    if no_improve_epoch > patience:\n",
    "        print(\"Early stopping!\")\n",
    "        break\n",
    "\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss over Time\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "involved-insulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test with Validation set\n",
    "model.eval()\n",
    "all_predicted_top1 = []\n",
    "all_predicted_top5 = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    total = 0\n",
    "    correct_top1 = 0\n",
    "    correct_top5 = 0\n",
    "    for x_batch, y_batch in test_loader:\n",
    "        \n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        outputs = model(x_batch)\n",
    "        _, predicted_top1 = torch.max(outputs, 1)\n",
    "        _, predicted_top5 = outputs.topk(5, 1, True, True)\n",
    "        \n",
    "        total += y_batch.size(0)\n",
    "        correct_top1 += (predicted_top1 == y_batch).sum().item()\n",
    "        correct_top5 += predicted_top5.eq(y_batch.view(-1, 1).expand_as(predicted_top5)).sum().item()\n",
    "\n",
    "        \n",
    "        all_predicted_top1.extend(predicted_top1.cpu().numpy())\n",
    "        all_predicted_top5.extend(predicted_top5.cpu().numpy())\n",
    "        all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    print(f'Top-1 Accuracy: {correct_top1 / total * 100}%')\n",
    "    print(f'Top-5 Accuracy: {correct_top5 / total * 100}%')\n",
    "\n",
    "for i in range(len(all_labels)):\n",
    "    true_label = label_encoder.inverse_transform([all_labels[i]])[0]\n",
    "    predicted_label_top1 = label_encoder.inverse_transform([all_predicted_top1[i]])[0]\n",
    "    predicted_label_top5 = label_encoder.inverse_transform(all_predicted_top5[i])\n",
    "    print(\"True labels: \", true_label)\n",
    "    print(\"Predicted labels (Top-1): \", predicted_label_top1)\n",
    "    print(\"Predicted labels (Top-5): \", predicted_label_top5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secure-patent",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test model with test set\n",
    "with open('FTIR_test_data.pkl', 'rb') as f:\n",
    "    data_real = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-neighbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = data_real.iloc[:, 1:].values\n",
    "X_test_resampled = X_test[:, new_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funded-illinois",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resampled = pd.DataFrame(X_test_resampled, index=data_real.index)\n",
    "data_real_resampled = pd.concat([data_real.iloc[:, :1], df_resampled], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepted-bloom",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = data_real_resampled.iloc[:, 1:].values\n",
    "y1 = data_real_resampled.iloc[:, 0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affecting-deposit",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels_test = np.unique(y1)\n",
    "\n",
    "missing_labels = set(unique_labels_test).difference(set(unique_labels))\n",
    "\n",
    "y1_series = pd.Series(y1)\n",
    "\n",
    "mask = ~y1_series.isin(missing_labels)\n",
    "X1 = X1[mask.values]\n",
    "y1 = y1_series[mask].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphic-office",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_encoded = label_encoder.transform(y1)\n",
    "X1 = torch.tensor(X1, dtype=torch.float)\n",
    "y1 = torch.tensor(y_encoded, dtype=torch.long)\n",
    "batch_size = batch_size\n",
    "test_data1 = TensorDataset(X1, y1)\n",
    "test_loader_real = DataLoader(test_data1, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gorgeous-ticket",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_predicted_top1 = []\n",
    "all_predicted_top3 = []\n",
    "all_predicted_top5 = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    total = 0\n",
    "    correct_top1 = 0\n",
    "    correct_top3 = 0\n",
    "    correct_top5 = 0\n",
    "    for x_batch, y_batch in test_loader_real:\n",
    "        \n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        outputs = model(x_batch)\n",
    "        _, predicted_top1 = torch.max(outputs, 1)\n",
    "        _, predicted_top3 = outputs.topk(3, 1, True, True)\n",
    "        _, predicted_top5 = outputs.topk(5, 1, True, True)\n",
    "        \n",
    "        total += y_batch.size(0)\n",
    "        correct_top1 += (predicted_top1 == y_batch).sum().item()\n",
    "        correct_top3 += predicted_top3.eq(y_batch.view(-1, 1).expand_as(predicted_top3)).sum().item()\n",
    "        correct_top5 += predicted_top5.eq(y_batch.view(-1, 1).expand_as(predicted_top5)).sum().item()\n",
    "\n",
    "    \n",
    "        all_predicted_top1.extend(predicted_top1.cpu().numpy())\n",
    "        all_predicted_top3.extend(predicted_top3.cpu().numpy())\n",
    "        all_predicted_top5.extend(predicted_top5.cpu().numpy())\n",
    "        all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    print(f'Top-1 Accuracy: {correct_top1 / total * 100}%')\n",
    "    print(f'Top-3 Accuracy: {correct_top3 / total * 100}%')\n",
    "    print(f'Top-5 Accuracy: {correct_top5 / total * 100}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diverse-jurisdiction",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37torch",
   "language": "python",
   "name": "py37torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
